{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed9eb255",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T05:47:06.839384Z",
     "iopub.status.busy": "2025-10-31T05:47:06.838471Z",
     "iopub.status.idle": "2025-10-31T05:48:30.118817Z",
     "shell.execute_reply": "2025-10-31T05:48:30.118009Z"
    },
    "papermill": {
     "duration": 83.285564,
     "end_time": "2025-10-31T05:48:30.120464",
     "exception": false,
     "start_time": "2025-10-31T05:47:06.834900",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/CLIP.git\r\n",
      "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-xsil005i\r\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-xsil005i\r\n",
      "  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\r\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "Collecting ftfy (from clip==1.0)\r\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\r\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (25.0)\r\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2025.9.18)\r\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\r\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\r\n",
      "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\r\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.19.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.15.0)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.9.0)\r\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->clip==1.0)\r\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->clip==1.0)\r\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->clip==1.0)\r\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->clip==1.0)\r\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->clip==1.0)\r\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->clip==1.0)\r\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->clip==1.0)\r\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->clip==1.0)\r\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->clip==1.0)\r\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\r\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->clip==1.0)\r\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\r\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\r\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (1.26.4)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2025.2.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2022.2.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision->clip==1.0) (2.4.1)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision->clip==1.0) (2022.2.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->clip==1.0) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision->clip==1.0) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision->clip==1.0) (2024.2.0)\r\n",
      "Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hBuilding wheels for collected packages: clip\r\n",
      "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=5799742a9992884047c5f5eaf2fc47e7caa1326b3c7767ddcd6333d7d1f34f59\r\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-pnbtawza/wheels/3f/7c/a4/9b490845988bf7a4db33674d52f709f088f64392063872eb9a\r\n",
      "Successfully built clip\r\n",
      "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ftfy, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, clip\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\r\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\r\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\r\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\r\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed clip-1.0 ftfy-6.3.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\r\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78fe1124",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-31T05:48:30.167970Z",
     "iopub.status.busy": "2025-10-31T05:48:30.167486Z",
     "iopub.status.idle": "2025-10-31T05:48:40.939737Z",
     "shell.execute_reply": "2025-10-31T05:48:40.938883Z"
    },
    "papermill": {
     "duration": 10.79712,
     "end_time": "2025-10-31T05:48:40.940981",
     "exception": false,
     "start_time": "2025-10-31T05:48:30.143861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images: 20000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import os\n",
    "from os.path import join, basename\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from typing import List, Tuple, Any\n",
    "import gc\n",
    "\n",
    "dataset_path = \"/kaggle/input/lost-in-the-museum-v2/archive/kaggle_dataset/kaggle_dataset\"\n",
    "\n",
    "all_images = sorted(glob(join(dataset_path, \"*.png\")))\n",
    "print(f\"All images: {len(all_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f69c295e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T05:48:40.987755Z",
     "iopub.status.busy": "2025-10-31T05:48:40.987403Z",
     "iopub.status.idle": "2025-10-31T05:48:41.044079Z",
     "shell.execute_reply": "2025-10-31T05:48:41.043279Z"
    },
    "papermill": {
     "duration": 0.081257,
     "end_time": "2025-10-31T05:48:41.045196",
     "exception": false,
     "start_time": "2025-10-31T05:48:40.963939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82764b4a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T05:48:41.091036Z",
     "iopub.status.busy": "2025-10-31T05:48:41.090790Z",
     "iopub.status.idle": "2025-10-31T05:48:58.276852Z",
     "shell.execute_reply": "2025-10-31T05:48:58.276043Z"
    },
    "papermill": {
     "duration": 17.210247,
     "end_time": "2025-10-31T05:48:58.278069",
     "exception": false,
     "start_time": "2025-10-31T05:48:41.067822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 890M/890M [00:03<00:00, 250MiB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CLIP(\n",
       "  (visual): VisionTransformer(\n",
       "    (conv1): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14), bias=False)\n",
       "    (ln_pre): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (transformer): Transformer(\n",
       "      (resblocks): Sequential(\n",
       "        (0): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (2): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (3): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (4): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (5): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (6): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (7): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (8): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (9): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (10): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (11): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (12): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (13): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (14): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (15): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (16): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (17): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (18): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (19): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (20): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (21): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (22): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (23): ResidualAttentionBlock(\n",
       "          (attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): Sequential(\n",
       "            (c_fc): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (gelu): QuickGELU()\n",
       "            (c_proj): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          )\n",
       "          (ln_2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_post): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (resblocks): Sequential(\n",
       "      (0): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (2): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (3): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (4): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (5): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (6): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (7): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (8): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (9): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (10): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (11): ResidualAttentionBlock(\n",
       "        (attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Sequential(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): QuickGELU()\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (token_embedding): Embedding(49408, 768)\n",
       "  (ln_final): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import clip\n",
    "\n",
    "# OpenCLIP (ViT-L/14)\n",
    "# We use the OpenCLIP library directly for this.\n",
    "CLIP_MODEL_NAME = \"ViT-L/14\"\n",
    "CLIP_PRETRAINED_DATASET = \"laion2b_s34b_b88k\"\n",
    "clip_model, clip_preprocess = clip.load(\n",
    "    CLIP_MODEL_NAME, \n",
    "    device=\"cpu\", # Will be moved to the target device later\n",
    "    jit=False, \n",
    "    download_root='/kaggle/temp/clip_cache/' # Use temp directory for Kaggle\n",
    ")\n",
    "clip_model.to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3af98518",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T05:48:58.327770Z",
     "iopub.status.busy": "2025-10-31T05:48:58.327509Z",
     "iopub.status.idle": "2025-10-31T05:49:17.734747Z",
     "shell.execute_reply": "2025-10-31T05:49:17.733799Z"
    },
    "papermill": {
     "duration": 19.433913,
     "end_time": "2025-10-31T05:49:17.736735",
     "exception": false,
     "start_time": "2025-10-31T05:48:58.302822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/facebookresearch/dinov2/zipball/main\" to /root/.cache/torch/hub/main.zip\n",
      "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/root/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/dinov2/dinov2_vitl14/dinov2_vitl14_reg4_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dinov2_vitl14_reg4_pretrain.pth\n",
      "100%|██████████| 1.13G/1.13G [00:12<00:00, 98.3MB/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DinoVisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 1024, kernel_size=(14, 14), stride=(14, 14))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-23): 24 x NestedTensorBlock(\n",
       "      (norm1): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): MemEffAttention(\n",
       "        (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "        (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): LayerScale()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "        (drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): LayerScale()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((1024,), eps=1e-06, elementwise_affine=True)\n",
       "  (head): Identity()\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DinoV2 (ViT-L/14)\n",
    "dinov2_model = torch.hub.load('facebookresearch/dinov2', 'dinov2_vitl14_reg')\n",
    "dinov2_model.to(DEVICE).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f2a202c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-31T05:49:17.827333Z",
     "iopub.status.busy": "2025-10-31T05:49:17.827020Z",
     "iopub.status.idle": "2025-10-31T06:31:50.267088Z",
     "shell.execute_reply": "2025-10-31T06:31:50.266300Z"
    },
    "papermill": {
     "duration": 2552.482961,
     "end_time": "2025-10-31T06:31:50.268414",
     "exception": false,
     "start_time": "2025-10-31T05:49:17.785453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All images found: 20000\n",
      "\n",
      "Starting feature extraction in chunks of 100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Chunks: 100%|██████████| 200/200 [42:32<00:00, 12.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✨ Feature extraction complete. Final output saved to /kaggle/working/submission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"/kaggle/input/lost-in-the-museum-v2/archive/kaggle_dataset/kaggle_dataset\"\n",
    "all_images = sorted(glob(join(dataset_path, \"*.png\")))\n",
    "print(f\"All images found: {len(all_images)}\")\n",
    "\n",
    "def load_image_raw(image_path: str):\n",
    "    \"\"\"Load a single image without initial transformation.\"\"\"\n",
    "    try:\n",
    "        return Image.open(image_path).convert('RGB')\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# DINOv2 Preprocessor (Standard ImageNet normalization and sizing)\n",
    "# ViT-B/14 typically uses 224x224 or 518x518. We use 224 for balance.\n",
    "DINOV2_RESIZE = 224 \n",
    "dinov2_preprocess = transforms.Compose([\n",
    "    transforms.Resize(DINOV2_RESIZE, interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.CenterCrop(DINOV2_RESIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "# --- 3. CHUNK-BASED FEATURE EXTRACTION FUNCTION ---\n",
    "\n",
    "def extract_features_chunk(\n",
    "    image_chunk: List[str], \n",
    "    dinov2_model: torch.nn.Module, \n",
    "    clip_model: torch.nn.Module, \n",
    "    clip_preprocess: Any, \n",
    "    device: str\n",
    ") -> Tuple[List[str], List[np.ndarray]]:\n",
    "    \"\"\"Extracts features for a list of images using DINOv2 and OpenCLIP.\"\"\"\n",
    "    \n",
    "    features_list = []\n",
    "    valid_image_paths = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, image_path in enumerate(image_chunk):\n",
    "            \n",
    "            raw_image = load_image_raw(image_path)\n",
    "            if raw_image is None:\n",
    "                continue\n",
    "            \n",
    "            # --- DINOv2 Feature Extraction ---\n",
    "            try:\n",
    "                dinov2_input = dinov2_preprocess(raw_image).unsqueeze(0).to(device)\n",
    "                \n",
    "                # Get patch tokens (including CLS and registers)\n",
    "                dinov2_output = dinov2_model.forward_features(dinov2_input)\n",
    "                # Take all patch tokens for GAP, typically in 'x_norm_patchtokens'\n",
    "                all_tokens = dinov2_output['x_norm_patchtokens'].squeeze(0) \n",
    "                \n",
    "                # Global Average Pooling (GAP) across all tokens for global feature\n",
    "                dinov2_feature = all_tokens.mean(dim=0)\n",
    "                dinov2_feature = dinov2_feature.cpu().numpy()\n",
    "                \n",
    "                # Cleanup PyTorch tensors\n",
    "                del dinov2_input, dinov2_output, all_tokens\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"DINOv2 error for {basename(image_path)}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # --- OpenCLIP Feature Extraction ---\n",
    "            try:\n",
    "                clip_input = clip_preprocess(raw_image).unsqueeze(0).to(device)\n",
    "                clip_feature = clip_model.encode_image(clip_input)\n",
    "                clip_feature = clip_feature.cpu().numpy().squeeze(0)\n",
    "                \n",
    "                # Cleanup PyTorch tensors\n",
    "                del clip_input\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"CLIP error for {basename(image_path)}: {e}\")\n",
    "                continue\n",
    "\n",
    "            # --- Normalization and Fusion ---\n",
    "            dinov2_feature_norm = dinov2_feature / np.linalg.norm(dinov2_feature)\n",
    "            clip_feature_norm = clip_feature / np.linalg.norm(clip_feature)\n",
    "            combined_features = np.concatenate([dinov2_feature_norm, clip_feature_norm])\n",
    "\n",
    "            # --- Store and Cleanup ---\n",
    "            features_list.append(combined_features)\n",
    "            valid_image_paths.append(basename(image_path))\n",
    "            \n",
    "            # Aggressive cleanup of raw data and intermediate numpy arrays\n",
    "            del dinov2_feature, clip_feature, dinov2_feature_norm, clip_feature_norm, combined_features\n",
    "            del raw_image\n",
    "            \n",
    "            # Periodically empty CUDA cache to clear fragmented VRAM\n",
    "            if (i + 1) % 50 == 0 and torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    return valid_image_paths, features_list\n",
    "\n",
    "# --- 4. MAIN CHUNKING AND SAVING LOGIC ---\n",
    "\n",
    "def run_feature_extraction_and_save(all_images: List[str], output_path: str, chunk_size: int = 500):\n",
    "    \"\"\"Iterates over images in chunks, extracts features, and appends to disk.\"\"\"\n",
    "\n",
    "    if os.path.exists(output_path):\n",
    "        os.remove(output_path)\n",
    "        print(f\"Existing file {output_path} removed.\")\n",
    "        \n",
    "    print(f\"\\nStarting feature extraction in chunks of {chunk_size}...\")\n",
    "    \n",
    "    total_images = len(all_images)\n",
    "    \n",
    "    for i, start_index in enumerate(tqdm(range(0, total_images, chunk_size), desc=\"Processing Chunks\")):\n",
    "        end_index = min(start_index + chunk_size, total_images)\n",
    "        image_chunk = all_images[start_index:end_index]\n",
    "        \n",
    "        # --- Extract features for the current chunk ---\n",
    "        valid_paths, features_list = extract_features_chunk(\n",
    "            image_chunk, \n",
    "            dinov2_model, clip_model, clip_preprocess, DEVICE\n",
    "        )\n",
    "        \n",
    "        if not features_list:\n",
    "            print(f\"Warning: Chunk {i+1} yielded no valid features. Skipping save.\")\n",
    "            continue\n",
    "            \n",
    "        # --- Create DataFrame and Save ---\n",
    "        N = features_list[0].shape[0]\n",
    "        column_names = [f\"feature_{i}\" for i in range(N)]\n",
    "        df = pd.DataFrame(features_list, columns=column_names)\n",
    "        df.insert(0, 'image_name', valid_paths)\n",
    "        df['ID'] = df['image_name']\n",
    "        \n",
    "        header = (i == 0) # Write header only for the first chunk\n",
    "        mode = 'w' if (i == 0) else 'a' # 'w' for first chunk, 'a' for append\n",
    "        \n",
    "        # Save to CSV, appending subsequent chunks\n",
    "        df.to_csv(output_path, mode=mode, header=header, index=False)\n",
    "        \n",
    "        # --- FINAL MEMORY CLEANUP FOR THE CHUNK ---\n",
    "        del features_list, valid_paths, df\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "# --- 5. EXECUTION ---\n",
    "output_csv_path = \"/kaggle/working/submission.csv\"\n",
    "CHUNK_SIZE = 100 # Keep this low (e.g., 100-500) until stable\n",
    "\n",
    "run_feature_extraction_and_save(all_images, output_csv_path, CHUNK_SIZE)\n",
    "\n",
    "print(f\"\\n✨ Feature extraction complete. Final output saved to {output_csv_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd3e0f2",
   "metadata": {
    "papermill": {
     "duration": 0.035551,
     "end_time": "2025-10-31T06:31:50.341860",
     "exception": false,
     "start_time": "2025-10-31T06:31:50.306309",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 14246410,
     "sourceId": 118964,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2690.071032,
   "end_time": "2025-10-31T06:31:52.945315",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-10-31T05:47:02.874283",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
