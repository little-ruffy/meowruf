{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00b61012",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T21:42:52.506039Z",
     "iopub.status.busy": "2025-09-22T21:42:52.505510Z",
     "iopub.status.idle": "2025-09-22T21:42:52.513148Z",
     "shell.execute_reply": "2025-09-22T21:42:52.512392Z"
    },
    "papermill": {
     "duration": 0.012123,
     "end_time": "2025-09-22T21:42:52.514246",
     "exception": false,
     "start_time": "2025-09-22T21:42:52.502123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing constants_rag.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile constants_rag.py\n",
    "\n",
    "# --- Model Paths ---\n",
    "# Model for retrieving similar examples (Encoder)\n",
    "EMBEDDING_MODEL_PATH = \"/kaggle/input/qwen-3-embedding/transformers/0.6b/1\"\n",
    "\n",
    "# Model for reasoning and final classification (Decoder)\n",
    "GENERATIVE_MODEL_PATH = \"/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1\"\n",
    "\n",
    "# --- Data Path ---\n",
    "DATA_PATH = \"/kaggle/input/jigsaw-agile-community-rules/\"\n",
    "\n",
    "# --- RAG Hyperparameters ---\n",
    "# --- NEW: Parameters for Similarity Spectrum strategy ---\n",
    "K_SPECTRUM_SHOTS = 5 # Total examples for each label (positive/negative)\n",
    "N_SPECTRUM_CANDIDATES = 50 # Initial larger pool of candidates to search from\n",
    "\n",
    "# Define the ranks/indices to pick from the N_SPECTRUM_CANDIDATES pool.\n",
    "# This should be a list of K_SPECTRUM_SHOTS distinct indices within [0, N_SPECTRUM_CANDIDATES-1].\n",
    "# Example: [0, 4, 9, 19, 29] for 5 shots from a pool of 30, picking roughly at 0%, 15%, 30%, 60%, 90%\n",
    "SPECTRUM_RANKS_TO_PICK = [0, 5, 15, 25, 40] # 0-indexed ranks (1st, 6th, 16th, 26th, 41st most similar)\n",
    "\n",
    "# --- Inference ---\n",
    "# The two possible answers our LLM can give\n",
    "POSITIVE_ANSWER = \"Yes\"\n",
    "NEGATIVE_ANSWER = \"No\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f32a8641",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T21:42:52.519515Z",
     "iopub.status.busy": "2025-09-22T21:42:52.519319Z",
     "iopub.status.idle": "2025-09-22T21:42:52.524244Z",
     "shell.execute_reply": "2025-09-22T21:42:52.523596Z"
    },
    "papermill": {
     "duration": 0.008702,
     "end_time": "2025-09-22T21:42:52.525311",
     "exception": false,
     "start_time": "2025-09-22T21:42:52.516609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing utils_rag.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile utils_rag.py\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def build_corpus(data_path):\n",
    "    \"\"\"\n",
    "    Builds a comprehensive corpus on-the-fly from train.csv and all examples in test.csv.\n",
    "    This serves as our knowledge base for retrieval.\n",
    "    \"\"\"\n",
    "    # This function remains unchanged.\n",
    "    print(\"Building corpus from all available labeled data...\")\n",
    "    train_df = pd.read_csv(f\"{data_path}/train.csv\")\n",
    "    test_df = pd.read_csv(f\"{data_path}/test.csv\")\n",
    "    corpus_data = []\n",
    "    for _, row in train_df.iterrows():\n",
    "        if pd.notna(row['body']):\n",
    "            corpus_data.append({'text': row['body'], 'rule': row['rule'], 'label': int(row['rule_violation'])})\n",
    "        for i in range(1, 3):\n",
    "            if pd.notna(row[f'positive_example_{i}']):\n",
    "                corpus_data.append({'text': row[f'positive_example_{i}'], 'rule': row['rule'], 'label': 1})\n",
    "            if pd.notna(row[f'negative_example_{i}']):\n",
    "                corpus_data.append({'text': row[f'negative_example_{i}'], 'rule': row['rule'], 'label': 0})\n",
    "    for _, row in test_df.iterrows():\n",
    "        for i in range(1, 3):\n",
    "            if pd.notna(row[f'positive_example_{i}']):\n",
    "                corpus_data.append({'text': row[f'positive_example_{i}'], 'rule': row['rule'], 'label': 1})\n",
    "            if pd.notna(row[f'negative_example_{i}']):\n",
    "                corpus_data.append({'text': row[f'negative_example_{i}'], 'rule': row['rule'], 'label': 0})\n",
    "    corpus_df = pd.DataFrame(corpus_data)\n",
    "    corpus_df = corpus_df.drop_duplicates(subset=['text', 'rule']).reset_index(drop=True)\n",
    "    print(f\"Corpus built successfully with {len(corpus_df)} total examples.\")\n",
    "    return corpus_df\n",
    "\n",
    "\n",
    "# --- REVERTED TO A SIMPLER PROMPT BUILDER ---\n",
    "def build_dynamic_prompt_similarity_spectrum(test_row, retrieved_pos_examples, retrieved_neg_examples):\n",
    "    \"\"\"\n",
    "    Constructs a detailed, dynamic prompt for the LLM using a spectrum of similarity examples.\n",
    "    \"\"\"\n",
    "    positive_examples_str = \"\"\n",
    "    if retrieved_pos_examples:\n",
    "        positive_examples_str += \"Here are several positive examples (violates the rule) covering a range of similarities to the comment being evaluated:\\n---\\n\"\n",
    "        for i, example in enumerate(retrieved_pos_examples):\n",
    "            positive_examples_str += f\"Example {i+1}:\\n'{example}'\\n\\n\"\n",
    "    else:\n",
    "        positive_examples_str = \"No suitable positive examples were found.\\n\\n\"\n",
    "\n",
    "    negative_examples_str = \"\"\n",
    "    if retrieved_neg_examples:\n",
    "        negative_examples_str += \"Here are several negative examples (does NOT violate the rule) covering a range of similarities to the comment being evaluated:\\n---\\n\"\n",
    "        for i, example in enumerate(retrieved_neg_examples):\n",
    "            negative_examples_str += f\"Example {i+1}:\\n'{example}'\\n\\n\"\n",
    "    else:\n",
    "        negative_examples_str = \"No suitable negative examples were found.\\n\\n\"\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "You are an expert Reddit content moderator. Your task is to determine if a comment violates a specific subreddit rule. You will be provided with examples that demonstrate the rule, chosen to cover a spectrum of relevance.\n",
    "\n",
    "**Subreddit:** r/{test_row['subreddit']}\n",
    "**Rule to Evaluate:** \"{test_row['rule']}\"\n",
    "\n",
    "{positive_examples_str}\n",
    "{negative_examples_str}\n",
    "---\n",
    "Now, carefully evaluate the following comment based on the rule and the examples provided above.\n",
    "\n",
    "**Comment to Classify:**\n",
    "\"{test_row['body']}\"\n",
    "\n",
    "Does this comment violate the rule? Respond with only \"Yes\" or \"No\".\n",
    "Answer:\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b951521",
   "metadata": {
    "papermill": {
     "duration": 0.001681,
     "end_time": "2025-09-22T21:42:52.528930",
     "exception": false,
     "start_time": "2025-09-22T21:42:52.527249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f23d0ce2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T21:42:52.533764Z",
     "iopub.status.busy": "2025-09-22T21:42:52.533468Z",
     "iopub.status.idle": "2025-09-22T21:42:52.539954Z",
     "shell.execute_reply": "2025-09-22T21:42:52.539275Z"
    },
    "papermill": {
     "duration": 0.010232,
     "end_time": "2025-09-22T21:42:52.541068",
     "exception": false,
     "start_time": "2025-09-22T21:42:52.530836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing inference_rag.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference_rag.py\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import vllm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import semantic_search\n",
    "from logits_processor_zoo.vllm import MultipleChoiceLogitsProcessor\n",
    "from scipy.special import softmax\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "from constants_rag import (\n",
    "    EMBEDDING_MODEL_PATH, GENERATIVE_MODEL_PATH, DATA_PATH, \n",
    "    K_SPECTRUM_SHOTS, N_SPECTRUM_CANDIDATES, SPECTRUM_RANKS_TO_PICK, # Import new params\n",
    "    POSITIVE_ANSWER, NEGATIVE_ANSWER\n",
    ")\n",
    "# Import the new prompt builder\n",
    "from utils_rag import build_corpus, build_dynamic_prompt_similarity_spectrum\n",
    "\n",
    "def main():\n",
    "    # --- Phase 1: Dynamic Setup & Indexing (No changes here) ---\n",
    "    print(\"--- Phase 1: Building corpus and search indexes ---\")\n",
    "    \n",
    "    embed_model = SentenceTransformer(EMBEDDING_MODEL_PATH, device=\"cuda\")\n",
    "    corpus_df = build_corpus(DATA_PATH)\n",
    "    \n",
    "    print(\"Creating rule-aware text for retrieval...\")\n",
    "    corpus_df['retrieval_text'] = \"Rule: \" + corpus_df['rule'] + \"\\nComment: \" + corpus_df['text']\n",
    "    \n",
    "    print(\"Computing rule-aware embeddings for the corpus...\")\n",
    "    corpus_embeddings = embed_model.encode(\n",
    "        corpus_df['retrieval_text'].tolist(),\n",
    "        batch_size=128,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_tensor=True,\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "\n",
    "    print(\"Creating rule-specific search indexes in memory...\")\n",
    "    rule_indexes = {}\n",
    "    for rule in tqdm(corpus_df['rule'].unique(), desc=\"Indexing rules\"):\n",
    "        rule_mask = corpus_df['rule'] == rule\n",
    "        rule_corpus = corpus_df[rule_mask]\n",
    "        rule_embeddings_tensor = corpus_embeddings[rule_mask.to_numpy().nonzero()[0]]\n",
    "        \n",
    "        rule_indexes[rule] = {\n",
    "            'corpus_df': rule_corpus.reset_index(drop=True),\n",
    "            'embeddings': rule_embeddings_tensor\n",
    "        }\n",
    "    print(\"--- Phase 1 complete ---\")\n",
    "\n",
    "    # --- Phase 2: Inference Loop with Retrieval ---\n",
    "    print(\"\\n--- Phase 2: Generating prompts with Similarity Spectrum examples ---\")\n",
    "    \n",
    "    test_df = pd.read_csv(f\"{DATA_PATH}/test.csv\")\n",
    "    prompts = []\n",
    "\n",
    "    for _, test_row in tqdm(test_df.iterrows(), total=len(test_df), desc=\"Retrieving spectrum examples and building prompts\"):\n",
    "        query_text = test_row['body']\n",
    "        rule = test_row['rule']\n",
    "        \n",
    "        retrieved_pos_examples, retrieved_neg_examples = [], []\n",
    "        \n",
    "        if rule in rule_indexes:\n",
    "            index = rule_indexes[rule]\n",
    "            \n",
    "            query_retrieval_text = f\"Rule: {rule}\\nComment: {query_text}\"\n",
    "            query_embedding = embed_model.encode(query_retrieval_text, convert_to_tensor=True, device=\"cuda\")\n",
    "            \n",
    "            # --- START OF NEW SIMILARITY SPECTRUM LOGIC ---\n",
    "            \n",
    "            pos_mask = index['corpus_df']['label'] == 1\n",
    "            neg_mask = index['corpus_df']['label'] == 0\n",
    "            \n",
    "            # --- Handle Positive Examples ---\n",
    "            if pos_mask.any():\n",
    "                pos_corpus = index['corpus_df'][pos_mask]\n",
    "                pos_embeddings = index['embeddings'][pos_mask.to_numpy()]\n",
    "                \n",
    "                # 1. Retrieve a large pool of N candidates\n",
    "                all_pos_hits = semantic_search(query_embedding, pos_embeddings, top_k=N_SPECTRUM_CANDIDATES)[0]\n",
    "                \n",
    "                # 2. Select specific ranks from this pool\n",
    "                selected_pos_texts = []\n",
    "                for rank_idx in SPECTRUM_RANKS_TO_PICK:\n",
    "                    if rank_idx < len(all_pos_hits): # Ensure the rank exists in the retrieved hits\n",
    "                        hit = all_pos_hits[rank_idx]\n",
    "                        selected_pos_texts.append(pos_corpus.iloc[hit['corpus_id']]['text'])\n",
    "                retrieved_pos_examples = selected_pos_texts\n",
    "            \n",
    "            # --- Handle Negative Examples ---\n",
    "            if neg_mask.any():\n",
    "                neg_corpus = index['corpus_df'][neg_mask]\n",
    "                neg_embeddings = index['embeddings'][neg_mask.to_numpy()]\n",
    "                \n",
    "                # 1. Retrieve a large pool of N candidates\n",
    "                all_neg_hits = semantic_search(query_embedding, neg_embeddings, top_k=N_SPECTRUM_CANDIDATES)[0]\n",
    "                \n",
    "                # 2. Select specific ranks from this pool\n",
    "                selected_neg_texts = []\n",
    "                for rank_idx in SPECTRUM_RANKS_TO_PICK:\n",
    "                    if rank_idx < len(all_neg_hits): # Ensure the rank exists in the retrieved hits\n",
    "                        hit = all_neg_hits[rank_idx]\n",
    "                        selected_neg_texts.append(neg_corpus.iloc[hit['corpus_id']]['text'])\n",
    "                retrieved_neg_examples = selected_neg_texts\n",
    "\n",
    "            # --- END OF NEW LOGIC ---\n",
    "\n",
    "        # Use the new prompt builder\n",
    "        prompt = build_dynamic_prompt_similarity_spectrum(test_row, retrieved_pos_examples, retrieved_neg_examples)\n",
    "        prompts.append(prompt)\n",
    "\n",
    "    print(\"--- Phase 2 complete ---\")\n",
    "    \n",
    "    # --- Phase 3 & 4 (No changes here) ---\n",
    "    print(\"\\n--- Freeing VRAM by unloading the embedding model ---\")\n",
    "    del embed_model, corpus_embeddings, rule_indexes, corpus_df\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"VRAM freed.\")\n",
    "\n",
    "    print(\"\\n--- Phase 3: Running LLM inference ---\")\n",
    "\n",
    "    llm = vllm.LLM(\n",
    "        GENERATIVE_MODEL_PATH,\n",
    "        quantization='awq',\n",
    "        tensor_parallel_size=2,\n",
    "        gpu_memory_utilization=0.90,\n",
    "        trust_remote_code=True,\n",
    "        dtype=\"half\",\n",
    "        enforce_eager=True,\n",
    "        disable_log_stats=True,\n",
    "        max_model_len=4096,\n",
    "        disable_custom_all_reduce=True\n",
    "    )\n",
    "    tokenizer = llm.get_tokenizer()\n",
    "    mclp = MultipleChoiceLogitsProcessor(tokenizer, choices=[POSITIVE_ANSWER, NEGATIVE_ANSWER])\n",
    "    outputs = llm.generate(\n",
    "        prompts,\n",
    "        vllm.SamplingParams(\n",
    "            skip_special_tokens=True,\n",
    "            max_tokens=1,\n",
    "            logits_processors=[mclp],\n",
    "            logprobs=2, \n",
    "            temperature=0.0,\n",
    "        ),\n",
    "        use_tqdm=True,\n",
    "    )\n",
    "    print(\"--- Phase 3 complete ---\")\n",
    "\n",
    "    print(\"\\n--- Phase 4: Processing results and creating submission file ---\")\n",
    "    \n",
    "    logprob_yes_scores = []\n",
    "    \n",
    "    for out in outputs:\n",
    "        logprobs_dict = {lp.decoded_token: lp.logprob for lp in out.outputs[0].logprobs[0].values()} if out.outputs[0].logprobs else {}\n",
    "        logprob_yes = logprobs_dict.get(POSITIVE_ANSWER, -10.0)\n",
    "        logprob_yes_scores.append(logprob_yes)\n",
    "\n",
    "    submission_df = pd.DataFrame({\n",
    "        'row_id': test_df['row_id'],\n",
    "        'rule_violation': logprob_yes_scores\n",
    "    })\n",
    "    \n",
    "    submission_df['rule_violation'] = submission_df['rule_violation'].rank(pct=True)\n",
    "    submission_df.to_csv(\"submission.csv\", index=False)\n",
    "    \n",
    "    print(\"\\n✅ submission.csv created successfully!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    os.environ[\"VLLM_USE_V1\"] = \"0\"\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf1aabc7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T21:42:52.546033Z",
     "iopub.status.busy": "2025-09-22T21:42:52.545516Z",
     "iopub.status.idle": "2025-09-22T21:47:24.047161Z",
     "shell.execute_reply": "2025-09-22T21:47:24.046194Z"
    },
    "papermill": {
     "duration": 271.505822,
     "end_time": "2025-09-22T21:47:24.048843",
     "exception": false,
     "start_time": "2025-09-22T21:42:52.543021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-22 21:43:04.721042: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1758577385.046481     217 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1758577385.138137     217 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "--- Phase 1: Building corpus and search indexes ---\r\n",
      "Building corpus from all available labeled data...\r\n",
      "Corpus built successfully with 1875 total examples.\r\n",
      "Creating rule-aware text for retrieval...\r\n",
      "Computing rule-aware embeddings for the corpus...\r\n",
      "Batches: 100%|██████████████████████████████████| 15/15 [00:49<00:00,  3.29s/it]\r\n",
      "Creating rule-specific search indexes in memory...\r\n",
      "Indexing rules: 100%|█████████████████████████████| 2/2 [00:00<00:00,  3.98it/s]\r\n",
      "--- Phase 1 complete ---\r\n",
      "\r\n",
      "--- Phase 2: Generating prompts with Similarity Spectrum examples ---\r\n",
      "Retrieving spectrum examples and building prompts:   0%| | 0/10 [00:00<?, ?it/s]\r\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 14.28it/s]\r\n",
      "Retrieving spectrum examples and building prompts:  10%| | 1/10 [00:00<00:00,  9\r\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 22.28it/s]\r\n",
      "\r\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 24.17it/s]\r\n",
      "Retrieving spectrum examples and building prompts:  30%|▎| 3/10 [00:00<00:00, 14\r\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 24.54it/s]\r\n",
      "\r\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 24.79it/s]\r\n",
      "\r\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 24.85it/s]\r\n",
      "Retrieving spectrum examples and building prompts:  60%|▌| 6/10 [00:00<00:00, 18\r\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 24.45it/s]\r\n",
      "\r\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 24.78it/s]\r\n",
      "\r\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 24.69it/s]\r\n",
      "Retrieving spectrum examples and building prompts:  90%|▉| 9/10 [00:00<00:00, 19\r\n",
      "Batches: 100%|████████████████████████████████████| 1/1 [00:00<00:00, 24.50it/s]\r\n",
      "Retrieving spectrum examples and building prompts: 100%|█| 10/10 [00:00<00:00, 1\r\n",
      "--- Phase 2 complete ---\r\n",
      "\r\n",
      "--- Freeing VRAM by unloading the embedding model ---\r\n",
      "VRAM freed.\r\n",
      "\r\n",
      "--- Phase 3: Running LLM inference ---\r\n",
      "INFO 09-22 21:44:21 [__init__.py:235] Automatically detected platform cuda.\r\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\r\n",
      "INFO 09-22 21:44:36 [config.py:1604] Using max model len 4096\r\n",
      "WARNING 09-22 21:44:37 [config.py:1084] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n",
      "WARNING 09-22 21:44:38 [cuda.py:103] To see benefits of async output processing, enable CUDA graph. Since, enforce-eager is enabled, async output processor cannot be used\r\n",
      "INFO 09-22 21:44:38 [llm_engine.py:228] Initializing a V0 LLM engine (v0.10.0) with config: model='/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1', speculative_config=None, tokenizer='/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=awq, enforce_eager=True, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='xgrammar', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=None, served_model_name=/kaggle/input/qwen2.5/transformers/14b-instruct-awq/1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":false,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":0,\"local_cache_dir\":null}, use_cached_outputs=False, \r\n",
      "WARNING 09-22 21:44:38 [__init__.py:2899] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/usage/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\r\n",
      "WARNING 09-22 21:44:38 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 2 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\r\n",
      "INFO 09-22 21:44:38 [cuda.py:346] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "INFO 09-22 21:44:38 [cuda.py:395] Using XFormers backend.\r\n",
      "2025-09-22 21:44:44.804270: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\r\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\r\n",
      "E0000 00:00:1758577484.826918     254 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\r\n",
      "E0000 00:00:1758577484.833623     254 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\r\n",
      "INFO 09-22 21:44:48 [__init__.py:235] Automatically detected platform cuda.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=254)\u001b[0;0m INFO 09-22 21:44:49 [multiproc_worker_utils.py:226] Worker ready; awaiting tasks\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=254)\u001b[0;0m INFO 09-22 21:44:49 [cuda.py:346] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=254)\u001b[0;0m INFO 09-22 21:44:49 [cuda.py:395] Using XFormers backend.\r\n",
      "[W922 21:45:00.656843303 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W922 21:45:00.124165880 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W922 21:45:10.667417784 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "[W922 21:45:20.677751808 socket.cpp:200] [c10d] The hostname of the client socket cannot be retrieved. err=-3\r\n",
      "INFO 09-22 21:45:20 [__init__.py:1375] Found nccl from library libnccl.so.2\r\n",
      "INFO 09-22 21:45:20 [pynccl.py:70] vLLM is using nccl==2.26.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=254)\u001b[0;0m INFO 09-22 21:45:20 [__init__.py:1375] Found nccl from library libnccl.so.2\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=254)\u001b[0;0m INFO 09-22 21:45:20 [pynccl.py:70] vLLM is using nccl==2.26.2\r\n",
      "INFO 09-22 21:45:20 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_88eebab3'), local_subscribe_addr='ipc:///tmp/12195318-5f33-4830-a9aa-cb57df50b3fc', remote_subscribe_addr=None, remote_addr_ipv6=False)\r\n",
      "INFO 09-22 21:45:20 [parallel_state.py:1102] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=254)\u001b[0;0m INFO 09-22 21:45:20 [parallel_state.py:1102] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\r\n",
      "INFO 09-22 21:45:20 [model_runner.py:1083] Starting to load model /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1...\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=254)\u001b[0;0m INFO 09-22 21:45:21 [model_runner.py:1083] Starting to load model /kaggle/input/qwen2.5/transformers/14b-instruct-awq/1...\r\n",
      "Loading safetensors checkpoint shards:   0% Completed | 0/3 [00:00<?, ?it/s]\r\n",
      "Loading safetensors checkpoint shards:  33% Completed | 1/3 [00:11<00:22, 11.19s/it]\r\n",
      "Loading safetensors checkpoint shards:  67% Completed | 2/3 [00:41<00:22, 22.36s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [01:31<00:00, 35.16s/it]\r\n",
      "Loading safetensors checkpoint shards: 100% Completed | 3/3 [01:31<00:00, 30.58s/it]\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=254)\u001b[0;0m INFO 09-22 21:46:53 [default_loader.py:262] Loading weights took 91.37 seconds\r\n",
      "INFO 09-22 21:46:53 [default_loader.py:262] Loading weights took 92.09 seconds\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=254)\u001b[0;0m INFO 09-22 21:46:54 [model_runner.py:1115] Model loading took 4.6720 GiB and 91.613071 seconds\r\n",
      "INFO 09-22 21:46:54 [model_runner.py:1115] Model loading took 4.6717 GiB and 92.232870 seconds\r\n",
      "\r\n",
      "\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=254)\u001b[0;0m INFO 09-22 21:47:03 [worker.py:295] model weights take 4.67GiB; non_torch_memory takes 0.11GiB; PyTorch activation peak memory takes 0.39GiB; the rest of the memory reserved for KV Cache is 8.10GiB.\r\n",
      "\r\n",
      "\r\n",
      "INFO 09-22 21:47:04 [worker.py:295] model weights take 4.67GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 1.50GiB; the rest of the memory reserved for KV Cache is 7.02GiB.\r\n",
      "INFO 09-22 21:47:04 [executor_base.py:113] # cuda blocks: 4789, # CPU blocks: 2730\r\n",
      "INFO 09-22 21:47:04 [executor_base.py:118] Maximum concurrency for 4096 tokens per request: 18.71x\r\n",
      "INFO 09-22 21:47:10 [llm_engine.py:424] init engine (profile, create kv cache, warmup model) took 15.79 seconds\r\n",
      "Adding requests: 100%|█████████████████████████| 10/10 [00:00<00:00, 518.37it/s]\r\n",
      "Processed prompts: 100%|█| 10/10 [00:04<00:00,  2.17it/s, est. speed input: 1124\r\n",
      "--- Phase 3 complete ---\r\n",
      "\r\n",
      "--- Phase 4: Processing results and creating submission file ---\r\n",
      "\r\n",
      "✅ submission.csv created successfully!\r\n",
      "INFO 09-22 21:47:15 [multiproc_worker_utils.py:138] Terminating local vLLM worker processes\r\n",
      "\u001b[1;36m(VllmWorkerProcess pid=254)\u001b[0;0m INFO 09-22 21:47:15 [multiproc_worker_utils.py:260] Worker exiting\r\n",
      "[rank0]:[W922 21:47:19.208243160 ProcessGroupNCCL.cpp:1479] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\r\n"
     ]
    }
   ],
   "source": [
    "!python inference_rag.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8712718f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-22T21:47:24.063437Z",
     "iopub.status.busy": "2025-09-22T21:47:24.063164Z",
     "iopub.status.idle": "2025-09-22T21:47:24.200802Z",
     "shell.execute_reply": "2025-09-22T21:47:24.200000Z"
    },
    "papermill": {
     "duration": 0.147122,
     "end_time": "2025-09-22T21:47:24.202140",
     "exception": false,
     "start_time": "2025-09-22T21:47:24.055018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "row_id,rule_violation\r\n",
      "2029,0.1\r\n",
      "2030,0.3\r\n",
      "2031,0.95\r\n",
      "2032,0.8\r\n",
      "2033,0.6\r\n",
      "2034,0.5\r\n",
      "2035,0.95\r\n",
      "2036,0.2\r\n",
      "2037,0.4\r\n"
     ]
    }
   ],
   "source": [
    "!head submission.csv"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 13121456,
     "isSourceIdPinned": false,
     "sourceId": 94635,
     "sourceType": "competition"
    },
    {
     "datasetId": 8044304,
     "sourceId": 12726948,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8067935,
     "sourceId": 12762469,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 252850661,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 252853424,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 164048,
     "modelInstanceId": 141558,
     "sourceId": 166361,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 164048,
     "modelInstanceId": 145960,
     "sourceId": 171496,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 164048,
     "modelInstanceId": 146086,
     "sourceId": 171638,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 322000,
     "modelInstanceId": 310551,
     "sourceId": 375840,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 368803,
     "modelInstanceId": 347541,
     "sourceId": 426330,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 429004,
     "modelInstanceId": 411182,
     "sourceId": 523492,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 276.891177,
   "end_time": "2025-09-22T21:47:24.527787",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-22T21:42:47.636610",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
